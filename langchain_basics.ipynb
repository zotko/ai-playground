{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e574f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50367fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The grass is **green**.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM interface\n",
    "from langchain_google_genai.llms import GoogleGenerativeAI\n",
    "\n",
    "model = GoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "model.invoke(\"The grass is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93dda95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Germany is **Berlin**.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--325ee665-6eb6-45c1-84f3-7020c0286757-0', usage_metadata={'input_tokens': 7, 'output_tokens': 9, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat interface\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "prompt = [HumanMessage(content=\"What is the capital of Germany?\")]\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30168408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**The capital of Germany is Berlin.**', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--883f5e90-a28f-433c-82f1-34dba46dfe9b-0', usage_metadata={'input_tokens': 20, 'output_tokens': 9, 'total_tokens': 29, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Roles: HumanMessage, AIMessage, SystemMessage, ChatMessage\n",
    "system_msg = SystemMessage(\n",
    "    \"You are a helpful assistant who responds to questions using bold text.\"\n",
    ")\n",
    "human_msg = HumanMessage(\"What is the capital of Germany?\")\n",
    "prompt = [system_msg, human_msg]\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74e370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Summarize the following text in 1 sentence. \\nIf the text is too short or lacks meaningful content, respond with \"Not enough information to summarize.\"\\n\\nText: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications.\\n    As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\nSummary: ')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Templates make prompts reusable\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"\"\"Summarize the following text in 1 sentence. \n",
    "If the text is too short or lacks meaningful content, respond with \"Not enough information to summarize.\"\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Summary: \"\"\"\n",
    ")\n",
    "\n",
    "# Example input\n",
    "template.invoke(\n",
    "    {\n",
    "        \"text\": \"\"\"LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications.\n",
    "    As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\"\"\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fbd218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Use the following context to answer \\n    the question. If the answer is not available in the context, respond with \\n    \"Not enough information provided\".\\n\\nContext: Video games come in many genres. Action games focus on fast \\n        movement and quick reflexes. Puzzle games require players to think and \\n        solve problems. Role-playing games, or RPGs, let players explore worlds \\n        and complete quests as different characters.\\n\\nQuestion: What type of games involve solving problems?\\n\\nAnswer: ')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = PromptTemplate.from_template(\n",
    "    \"\"\"Use the following context to answer \n",
    "    the question. If the answer is not available in the context, respond with \n",
    "    \"Not enough information provided\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    ")\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"\"\"Video games come in many genres. Action games focus on fast \n",
    "        movement and quick reflexes. Puzzle games require players to think and \n",
    "        solve problems. Role-playing games, or RPGs, let players explore worlds \n",
    "        and complete quests as different characters.\"\"\",\n",
    "        \"question\": \"What type of games involve solving problems?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83355f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Puzzle games', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--c409d084-34c2-4785-963f-52a21c0da85e-0', usage_metadata={'input_tokens': 107, 'output_tokens': 3, 'total_tokens': 110, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23787f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Answer the question based on the context below. If the \\n        question cannot be answered using the information provided, answer with \\n        \"I don\\'t know\".', additional_kwargs={}, response_metadata={}), HumanMessage(content='Context: Modern gaming has evolved rapidly with the introduction of powerful graphics engines and online multiplayer capabilities. Popular platforms include PC, PlayStation, Xbox, and Nintendo Switch. Gamers often use services like Steam, PlayStation Network, and Xbox Live to access a vast library of titles and connect with friends around the world.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: Which platforms are popular for gaming?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Answer the question based on the context below. If the \n",
    "        question cannot be answered using the information provided, answer with \n",
    "        \"I don\\'t know\".\"\"\",\n",
    "        ),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"\"\"Modern gaming has evolved rapidly with the introduction of powerful graphics engines and online multiplayer capabilities. Popular platforms include PC, PlayStation, Xbox, and Nintendo Switch. Gamers often use services like Steam, PlayStation Network, and Xbox Live to access a vast library of titles and connect with friends around the world.\"\"\",\n",
    "        \"question\": \"Which platforms are popular for gaming?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730810e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='PC, PlayStation, Xbox, and Nintendo Switch.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--d710cfe1-a407-4b54-9ea5-ed9ced842a59-0', usage_metadata={'input_tokens': 104, 'output_tokens': 11, 'total_tokens': 115, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05000ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GamingAnswerWithJustification(answer='This information is not available.', justification='I do not have access to data regarding the popularity of PC versus console for online multiplayer gaming.')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JSON Output\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class GamingAnswerWithJustification(BaseModel):\n",
    "    \"\"\"An answer to the user's gaming question along with justification for the\n",
    "    answer.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    \"\"\"The answer to the user's gaming question\"\"\"\n",
    "    justification: str\n",
    "    \"\"\"Justification for the answer\"\"\"\n",
    "\n",
    "\n",
    "structured_llm = model.with_structured_output(GamingAnswerWithJustification)\n",
    "\n",
    "result = structured_llm.invoke(\n",
    "    \"\"\"Which platform is more popular for online multiplayer gaming, PC or console?\"\"\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ed82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"answer\":\"This information is not available.\",\"justification\":\"I do not have access to data regarding the popularity of PC versus console for online multiplayer gaming.\"}'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.model_dump_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b10717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'banana', 'cherry']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output parsing\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "parser.invoke(\"apple, banana, cherry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b17f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Hi there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--43a2f765-b3d6-428b-8571-d4e6421864d2-0', usage_metadata={'input_tokens': 2, 'output_tokens': 11, 'total_tokens': 13, 'input_token_details': {'cache_read': 0}}),\n",
       " AIMessage(content='Hallo! Wie kann ich dir heute helfen?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--9b68184f-5c86-413e-8ce6-6a5ffcd4abff-0', usage_metadata={'input_tokens': 2, 'output_tokens': 10, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}}),\n",
       " AIMessage(content=\"Bonjour ! Comment puis-je vous aider aujourd'hui ?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--112fc155-14fd-4449-acae-69e89876a669-0', usage_metadata={'input_tokens': 2, 'output_tokens': 13, 'total_tokens': 15, 'input_token_details': {'cache_read': 0}}),\n",
       " AIMessage(content='–ü—Ä–∏–≤—ñ—Ç! –ß–∏–º –º–æ–∂—É –¥–æ–ø–æ–º–æ–≥—Ç–∏? üòä', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--9c2789d1-4d9b-477f-bf1d-f9f871e5fa99-0', usage_metadata={'input_tokens': 3, 'output_tokens': 11, 'total_tokens': 14, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch processing\n",
    "model.batch([\"Hi!\", \"Hallo!\", \"Bonjour!\", \"–ü—Ä–∏–≤—ñ—Ç!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='I' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--b73fd203-a346-4ca5-b2a6-ff9b258a0572' usage_metadata={'input_tokens': 5, 'output_tokens': 0, 'total_tokens': 5, 'input_token_details': {'cache_read': 0}}\n",
      "content=' am doing well' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--b73fd203-a346-4ca5-b2a6-ff9b258a0572' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
      "content=\", thank you for asking! As a large language model, I don't\" additional_kwargs={} response_metadata={'safety_ratings': []} id='run--b73fd203-a346-4ca5-b2a6-ff9b258a0572' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
      "content=' experience emotions or physical sensations in the same way humans do, but I am functioning' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--b73fd203-a346-4ca5-b2a6-ff9b258a0572' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
      "content=' as intended and ready to assist you. How can I help you today?\\n' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--b73fd203-a346-4ca5-b2a6-ff9b258a0572' usage_metadata={'input_tokens': -1, 'output_tokens': 52, 'total_tokens': 51, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Streaming processing\n",
    "for token in model.stream(\"How are you?\"):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd5b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--1f690184-5871-4c3d-aacb-72aa6542992e-0', usage_metadata={'input_tokens': 13, 'output_tokens': 8, 'total_tokens': 21, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imperative composition\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    return model.invoke(prompt)\n",
    "\n",
    "\n",
    "chatbot.invoke({\"question\": \"What is the capital of France?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64d650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--f7d91889-8d17-4526-b424-1b33f92299c4' usage_metadata={'input_tokens': 14, 'output_tokens': 0, 'total_tokens': 14, 'input_token_details': {'cache_read': 0}}\n",
      "content=' capital of Greece' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--f7d91889-8d17-4526-b424-1b33f92299c4' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
      "content=' is Athens.\\n' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--f7d91889-8d17-4526-b424-1b33f92299c4' usage_metadata={'input_tokens': -1, 'output_tokens': 8, 'total_tokens': 7, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Streaming\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.stream(prompt):\n",
    "        yield token\n",
    "\n",
    "\n",
    "for part in chatbot.stream({\"question\": \"What is the capital of Greece?\"}):\n",
    "    print(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf861a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='South Africa does not have one capital city, but three! They are:\\n\\n*   **Pretoria** (executive capital)\\n*   **Cape Town** (legislative capital)\\n*   **Bloemfontein** (judicial capital)', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--a508e2b2-4e7a-4d00-90f8-71d280321ba9-0', usage_metadata={'input_tokens': 14, 'output_tokens': 50, 'total_tokens': 64, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asynchronous processing\n",
    "@chain\n",
    "async def chatbot(values):\n",
    "    prompt = await template.ainvoke(values)\n",
    "    return await model.ainvoke(prompt)\n",
    "\n",
    "\n",
    "await chatbot.ainvoke({\"question\": \"What is the capital of South Africa?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ffdead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Italy is Rome.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--8c147277-5bd4-42ca-b95f-b18a5189319a-0', usage_metadata={'input_tokens': 13, 'output_tokens': 8, 'total_tokens': 21, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declarative composition\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chatbot = template | model\n",
    "chatbot.invoke({\"question\": \"What is the capital of Italy?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55635b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The' additional_kwargs={} response_metadata={'safety_ratings': []} id='run--f9acd358-2bc0-4af2-850e-4d442eead70b' usage_metadata={'input_tokens': 14, 'output_tokens': 0, 'total_tokens': 14, 'input_token_details': {'cache_read': 0}}\n",
      "content=' capital of Spain is Madrid.\\n' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--f9acd358-2bc0-4af2-850e-4d442eead70b' usage_metadata={'input_tokens': -1, 'output_tokens': 8, 'total_tokens': 7, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "for part in chatbot.stream({\"question\": \"What is the capital of Spain?\"}):\n",
    "    print(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aaebbd",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3aed78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './waechtersbach.txt'}, page_content='Schloss W√§chtersbach stellt die Geburtsst√§tte unserer Stadt\\ndar. Von Kaiser Barbarossa als Jagd- und Sicherungsburg\\nseines Reichsforsts gegr√ºndet siedelten sich von hier aus\\nnach und nach Bauern und Bedienstete der Burg an.\\nIn ihrer Geschichte ver√§nderte sich nicht nur die Stadt, die\\nseit ihrer ersten geschichtlichen Erw√§hnung im Jahre 1236\\nstetig w√§chst. Auch die Burg selbst wurde seitdem in vielen\\nAbschnitten erweitert und letztendlich zum Schloss, in dem\\ndie f√ºrstliche Familie zu Ysenburg residierte.\\nDie Gr√ºndung der ‚ÄûWaechtersbacher Keramik‚Äú, die bis heute\\ninternational ber√ºhmt ist, hat letztlich ihren Ursprung im\\nhistorischen Zentrum von W√§chtersbach, da Graf Adolf als\\nMitbegr√ºnder der einzigartigen Manufaktur ihr den Namen\\nseiner Stadt verlieh.\\nZuletzt diente Schloss W√§chtersbach dem Deutschen Ent-\\nwicklungsdienst als Ausbildungsst√§tte. Seit dem Jahr 1978\\nstand das Schloss W√§chtersbach leer und war sich selbst\\n√ºberlassen.')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./waechtersbach.txt\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3fe177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.langchain.com/', 'title': 'LangChain', 'description': 'LangChain‚Äôs suite of products supports developers along each step of their development journey.', 'language': 'en'}, page_content=\"LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nFrameworksLangGraphLangChainPlatformsLangSmithLangGraph PlatformResources\\n\\nResources HubBlogCustomer StoriesLangChain AcademyCommunityExpertsChangelogDocs\\n\\nPythonLangGraphLangSmithLangChainJavaScriptLangGraphLangSmithLangChainCompany\\n\\nAboutCareersPricing\\n\\nLangSmithLangGraph PlatformGet a demoSign up\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nFrameworksLangGraphLangChainPlatformsLangSmithLangGraph PlatformResources\\n\\nResources HubBlogCustomer StoriesLangChain AcademyCommunityExpertsChangelogDocs\\n\\nPythonLangGraphLangSmithLangChainJavaScriptLangGraphLangSmithLangChainCompany\\n\\nAboutCareersPricing\\n\\nLangSmithLangGraph PlatformGet a demoSign upThe platform for reliable agents. Tools for every step of the agent development lifecycle -- built to unlock powerful AI\\xa0in production.Request a demoSee the docs\\n\\nLangChain products power top engineering teams, from startups to global enterprisesAccelerate agent development.Build faster with templates & a visual agent IDE. Reuse, configure, and combine agents to go further with less code.Ship reliable agents.Design agents that can handle sophisticated tasks with control. Add human-in-the-loop to steer and approve agent actions.Gain visibility & improve quality.See what‚Äôs happening - so you can quickly trace to root cause and debug issues. Evaluate your agent performance to improve over time.The Agent StackORCHESTRATION:Build agents with LangGraphControllable agent orchestration with built-in persistence to handle conversational history, memory, and agent-to-agent collaboration.INTEGRATIONS:Integrate components with LangChainIntegrate with the latest models, databases, and tools with no engineering overhead.EVALS\\xa0&\\xa0OBSERVABILITY:Gain visibility with LangSmithDebug poor-performing LLM app runs. Evaluate and observe agent performance at scale.DEPLOYMENT:Deploy &\\xa0manage with LangGraph PlatformDeploy and scale enterprise-grade agents with long-running workflows. Discover, reuse, and share agents across teams ‚Äî and iterate faster with LangGraph Studio.CopilotsBuild native co-pilots into your application to unlock new end user experiences for domain-specific tasks.Enterprise GPTGive all employees access\\u2028to information and tools\\u2028in a compliant manner so they\\u2028can perform their best.Customer SupportImprove the speed & efficiency\\u2028of support teams that handle customer requests.ResearchSynthesize data, summarize sources & uncover insights faster than ever for knowledge work.Code generationAccelerate software development by automating code writing, refactoring, and documentation for your team.AI SearchOffer a concierge experience to guide users to products or information in a personalized way.\\n\\n\\n\\n\\n\\nLangChain products are designed to be used independently or stack for multiplicative benefit. LangChainLangGraphFrameworksLangSmithLangGraph PlatformPlatformsFrameworksLangChainLangGraphPlatformsLangSmithLangGraph \\u2028PlatformSTACK 1:\\xa0LangGraph +\\xa0LangChain +\\xa0LangSmith +\\xa0LangGraph\\xa0PlatformA full product suite for reliable agents and LLM appsLangChain's products work seamlessly together to provide an integrated solution for every step of the application development journey. When you use all LangChain products, you'll build better, get to production quicker, and grow visibility -- all with less set up and friction. LangChain provides the smoothest path to high quality agents.Orchestration:Integrations:Evals + Observability:Deployment:STACK 2: No framework +\\xa0LangSmithTrace\\xa0and evaluate any LLM appLangSmith is framework-agnostic. Trace using the TypeScript or Python SDK\\xa0to gain visibility into your agent interactions -- whether you use LangChain's frameworks or not.Orchestration:Your choiceEvals + Observability:STACK 3:\\xa0Any agent framework +\\xa0LangGraph PlatformBuild agents any way you want, then deploy and scale with easeLangGraph Platform works with any agent framework, enabling stateful UXs like human-in-the-loop and streaming-native deployments.Orchestration:Your choiceDeployment:Get inspired by companies who have done it.Teams building with LangChain products are driving operational efficiency, increasing discovery & personalization, and delivering premium products that generate revenue.Discover Use Cases\\n\\n\\nFinancial ServicesKlarna's AI assistant has reduced average customer query resolution time by 80%, powered by LangSmith and LangGraph\\n\\n\\nTransportationThis global logistics provider is saving 600 hours a day using an automated order system built on LangGraph and LangSmith\\n\\n\\nSecurityAs a leading cybersecurity firm with 40k+ customers, Trellix cut log parsing from days to minutes using LangGraph and LangSmith.\\n\\n\\nThe biggest developer community in GenAILearn alongside the 1M+ practitioners using our frameworks to push the industry forward.#1Downloaded agent framework100k+GitHub stars#1Downloaded agent framework600+IntegrationsReady to start shipping \\u2028reliable agents faster?Get started with tools from the LangChain product suite for every step of the agent development lifecycle.Get a demoSign up for freeProductsLangChainLangSmithLangGraphAgentsEvaluationRetrievalResourcesPython DocsJS/TS DocsGitHubIntegrationsChangelogCommunityLangSmith Trust PortalCompanyAboutCareersBlogTwitterLinkedInYouTubeMarketing AssetsSign up for our newsletter to stay up to dateThank you! Your submission has been received!Oops! Something went wrong while submitting the form.All systems operationalPrivacy PolicyTerms of Service\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Web Loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.langchain.com/\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bedca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF Loader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"waechtersbach.pdf\")\n",
    "pages = loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab4e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Splitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"./waechtersbach.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "splitted_docs = splitter.split_documents(docs)\n",
    "len(splitted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19282da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Splitter for programming languages\n",
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "code = \"\"\"\n",
    "def faktorial(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * faktorial(n - 1)\n",
    "faktorial(5)\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([code])\n",
    "len(python_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d74243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.langchain.com'}, page_content='LangChain is a framework for building LLM-powered')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import Language\n",
    "\n",
    "markdown_text = \"\"\"\n",
    "LangChain is a framework for building LLM-powered applications. It helps you chain\n",
    "together interoperable components and third-party integrations to simplify AI\n",
    "application development ‚Äî  all while future-proofing decisions as the underlying\n",
    "technology evolves.\n",
    "\n",
    "```bash\n",
    "pip install -U langchain\n",
    "```\n",
    "\n",
    "To learn more about LangChain, check out\n",
    "[the docs](https://python.langchain.com/docs/introduction/). If you‚Äôre looking for more\n",
    "advanced customization or agent orchestration, check out\n",
    "[LangGraph](https://langchain-ai.github.io/langgraph/), our framework for building\n",
    "controllable agent workflows.\n",
    "\"\"\"\n",
    "\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "md_docs = md_splitter.create_documents(\n",
    "    [markdown_text], [{\"source\": \"https://www.langchain.com\"}]\n",
    ")\n",
    "md_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ec55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.013569727540016174,\n",
       " 0.007698915898799896,\n",
       " 0.004195104818791151,\n",
       " -0.0801468938589096,\n",
       " -0.009787925519049168,\n",
       " 0.0188269205391407,\n",
       " -0.01276074256747961,\n",
       " 0.010045362636446953,\n",
       " 0.021251028403639793,\n",
       " 0.006386178079992533]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text embeddings\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-exp-03-07\"\n",
    ")\n",
    "vector = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"LangChain is a framework for building LLM-powered applications.\",\n",
    "        \"It helps you chain together interoperable components and third-party integrations.\",\n",
    "        \"To learn more about LangChain, check out the docs.\",\n",
    "    ]\n",
    ")\n",
    "vector[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd59ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = PyPDFLoader(\"waechtersbach.pdf\").load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "chunks = text_splitter.split_documents(doc)\n",
    "\n",
    "embeddings = embeddings_model.embed_documents([chunk.page_content for chunk in chunks])\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fdb528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_postgres.vectorstores.PGVector at 0x7484c4474d90>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Database connection\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "raw_documents = PyPDFLoader(\"./waechtersbach.pdf\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "db = PGVector.from_documents(documents, embeddings_model, connection=connection)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dbf11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='95f73a40-ae3d-4809-9d9b-e586d08de111', metadata={'page': 0, 'source': './waechtersbach.pdf', 'creator': 'Adobe InDesign CC 14.0 (Macintosh)', 'moddate': '2019-05-17T08:37:38+02:00', 'trapped': '/False', 'producer': 'Adobe PDF Library 15.0', 'page_label': '1', 'total_pages': 8, 'creationdate': '2019-04-18T16:23:08+02:00'}, page_content='Meilenstein der\\nStadtgeschichte\\nWerden Sie'),\n",
       " Document(id='1af29204-3487-4a5b-96cd-79eb738ffbda', metadata={'page': 1, 'source': './waechtersbach.pdf', 'creator': 'Adobe InDesign CC 14.0 (Macintosh)', 'moddate': '2019-05-17T08:37:38+02:00', 'trapped': '/False', 'producer': 'Adobe PDF Library 15.0', 'page_label': '2', 'total_pages': 8, 'creationdate': '2019-04-18T16:23:08+02:00'}, page_content='Schloss W√§chtersbach stellt die Geburtsst√§tte unserer Stadt \\ndar. Von Kaiser Barbarossa als Jagd- und Sicherungsburg \\nseines Reichsforsts gegr√ºndet siedelten sich von hier aus \\nnach und nach Bauern und Bedienstete der Burg an.\\nIn ihrer Geschichte ver√§nderte sich nicht nur die Stadt, die \\nseit ihrer ersten geschichtlichen Erw√§hnung im Jahre 1236 \\nstetig w√§chst. Auch die Burg selbst wurde seitdem in vielen \\nAbschnitten erweitert und letztendlich zum Schloss, in dem  \\ndie f√ºrstliche Familie zu Ysenburg residierte.\\nDie Gr√ºndung der ‚ÄûWaechtersbacher Keramik‚Äú, die bis heute \\ninternational ber√ºhmt ist, hat letztlich ihren Ursprung im \\nhistorischen Zentrum von W√§chtersbach, da Graf Adolf als \\nMitbegr√ºnder der einzigartigen Manufaktur ihr den Namen \\nseiner Stadt verlieh. \\nZuletzt diente Schloss W√§chtersbach dem Deutschen Ent-\\nwicklungsdienst als Ausbildungsst√§tte. Seit dem Jahr 1978 \\nstand das Schloss W√§chtersbach leer und war sich selbst \\n√ºberlassen.'),\n",
       " Document(id='3ce28c2c-1965-450d-8db2-b5231f988f1d', metadata={'source': './waechtersbach.txt'}, page_content='Schloss W√§chtersbach stellt die Geburtsst√§tte unserer Stadt\\ndar. Von Kaiser Barbarossa als Jagd- und Sicherungsburg\\nseines Reichsforsts gegr√ºndet siedelten sich von hier aus\\nnach und nach Bauern und Bedienstete der Burg an.\\nIn ihrer Geschichte ver√§nderte sich nicht nur die Stadt, die\\nseit ihrer ersten geschichtlichen Erw√§hnung im Jahre 1236\\nstetig w√§chst. Auch die Burg selbst wurde seitdem in vielen\\nAbschnitten erweitert und letztendlich zum Schloss, in dem\\ndie f√ºrstliche Familie zu Ysenburg residierte.\\nDie Gr√ºndung der ‚ÄûWaechtersbacher Keramik‚Äú, die bis heute\\ninternational ber√ºhmt ist, hat letztlich ihren Ursprung im\\nhistorischen Zentrum von W√§chtersbach, da Graf Adolf als\\nMitbegr√ºnder der einzigartigen Manufaktur ihr den Namen\\nseiner Stadt verlieh.\\nZuletzt diente Schloss W√§chtersbach dem Deutschen Ent-\\nwicklungsdienst als Ausbildungsst√§tte. Seit dem Jahr 1978\\nstand das Schloss W√§chtersbach leer und war sich selbst\\n√ºberlassen.'),\n",
       " Document(id='0fbec918-f881-43dc-aa2f-124b174602c0', metadata={'page': 3, 'source': './waechtersbach.pdf', 'creator': 'Adobe InDesign CC 14.0 (Macintosh)', 'moddate': '2019-05-17T08:37:38+02:00', 'trapped': '/False', 'producer': 'Adobe PDF Library 15.0', 'page_label': '4', 'total_pages': 8, 'creationdate': '2019-04-18T16:23:08+02:00'}, page_content='Bereits im Jahr 2016 haben die Stadt W√§chtersbach und \\nder F√∂rderverein Schloss + Park 2001 e.V. ein Spenden-\\nkonto f√ºr die ‚ÄûSchloss- und Parkversch√∂nerung‚Äú einge-\\nrichtet, das der F√∂rderverein verwaltet und dessen Spen-\\ndengelder nicht in die Baufinanzierung flie√üen werden.\\nIn der Zwischenzeit h√§ufen sich die W√ºnsche vieler \\nMenschen, eine Spende einem besonderen Objekt im \\nSchloss zuordnen zu k√∂nnen, das dann dauerhaft ihren \\nNamen tr√§gt. Deshalb wird nun die bisherige Aktion \\nerweitert.\\nIn Zusammenarbeit mit dem F√∂rderverein Schloss + Park \\n2001 e.V. wird die Stadt W√§chtersbach daher dieses gro√üe \\nInteresse aufgreifen und ruft damit die Spendenaktion \\n‚ÄûWerden Sie ein Meilenstein der Stadtgeschichte‚Äú ins \\nLeben.\\nGeben Sie unserem Schloss eine Zukunft \\nSpenden Sie f√ºr diesen Meilenstein der Stadtgeschichte\\n‚Ä¢  F√ºr Ihre Spende erhalten Sie einen  \\n Stiftungsbrief\\n‚Ä¢  Ab 100 Euro verewigen wir Ihren  \\n Namen auf unserer Spendentafel im  \\n Innenhof von Schloss W√§chtersbach')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"Gr√ºndung\", k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b16a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
