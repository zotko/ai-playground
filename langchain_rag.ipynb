{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a289fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa13c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai.llms import GoogleGenerativeAI\n",
    "\n",
    "model = GoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6359302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './gemini.txt'}, page_content=\"An overview of the Gemini app\\nWe have long seen the potential of AI to make information and computing more accessible and useful to people. We have made pioneering advances on large language models (LLMs) and have seen great progress across Google and in this field more broadly. For several years, we have applied LLMs in the background to improve many of our products, such as autocompleting sentences in Gmail, expanding Google Translate, and helping us better understand queries in Google Search. We continue using LLMs for many Google services, as well as to power the Gemini app, which allows people to collaborate directly with generative AI. We want the Gemini app to be the most helpful and personal AI assistant, giving users direct access to Google’s latest AI models.\\n\\nWhile we’re at an important inflection point and encouraged by the widespread excitement around generative AI, it’s still early days for this technology. This explainer outlines how we’re approaching our work on the Gemini app (“Gemini”), including its mobile and web experiences — what it is, how it works and its current capabilities and limitations. Our approach to building Gemini will evolve as its underlying technology evolves, and as we learn from ongoing research, experience and user feedback.\\n\\nWhat Gemini is\\nGemini is an interface to a multimodal LLM (handling text, audio, images and more). Gemini is based on Google’s cutting-edge research in LLMs, which began with the Word2Vec paper in 2013 that proposed novel model architectures that mapped words as mathematical concepts, followed by the introduction of a neural conversational model in 2015. This framework demonstrated how models could predict the next sentence in a conversation based on the previous sentence or sentences, leading to more natural conversational experiences. This was followed by our breakthrough work on Transformer in 2017 and multi-turn chat capabilities in 2020, which demonstrated even more compelling generative language progress.\\n\\nWe initially launched Gemini (then called Bard) as an experiment in March 2023 in accordance with our AI Principles. Since then, users have turned to Gemini to write compelling emails, debug tricky coding problems, brainstorm ideas for upcoming events, get help learning difficult concepts, and so much more. Today, Gemini is a versatile AI tool that can help you in many ways. We already see Gemini helping people be more productive, more creative, and more curious and we add new functionality and innovations regularly.\\n\\nProductivity\\nFor starters, Gemini can save you time. For example, say you are looking to summarize a long research document; Gemini lets you upload it and gives you a useful synthesis. Gemini can also help with coding tasks, and coding has quickly become one of its most popular applications.\\n\\nCreativity\\nGemini can also help bring your ideas to life and spark your creativity. For example, if you’re writing a blog post, Gemini can create an outline and generate images that help illustrate your post. And coming soon with Gems, you will be able to customize Gemini with specific instructions and have it act as a subject matter expert to help you accomplish your personal goals.\\n\\nCuriosity\\nGemini can be a jumping off point for exploring your ideas and things you’d like to learn more about. For instance, it can explain a complex concept simply or surface relevant insights on a topic or image. And soon, it will pair these insights with recommended content from across the web to learn more about specific topics.\\n\\nGemini's capabilities are rapidly expanding -- soon, you’ll be able to point your phone’s camera at an object, say, for example, the Golden Gate bridge and ask Gemini to tell you about its paint color (if you’re wondering, it’s “International Orange”). You’ll also be able to ask Gemini to help you navigate a restaurant’s menu in another language and recommend a dish you’re likely to enjoy. These are just two examples of the new capabilities coming soon to Gemini.\\n\\nOf course we rigorously train and monitor Gemini so that its responses are likely to be reliable and aligned with your expectations. We also talk with industry experts, educators, policymakers, business leaders, civil and human rights leaders, and content creators to explore new applications, risks, and limitations of this emerging technology.\")]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./gemini.txt\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f2d21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://gemini.google/overview/', 'title': 'What is Gemini and how it works', 'description': \"Learn about Gemini: its capabilities, how it works, and its limitations. Explore Google's approach to AI, from model training to safety guidelines and future developments.\", 'language': 'en'}, page_content=\"\\n\\n\\n\\nWhat is Gemini and how it works\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat Gemini Can Do\\n\\n\\n\\n\\n\\n\\n\\nGemini Live\\n\\n\\nImage Generation\\n\\n\\nVideo Generation\\n\\n\\nDeep Research\\n\\n\\nPersonalization\\n\\n\\nCanvas\\n\\n\\nApps\\n\\n\\nGems\\n\\n\\nGemini in Chrome\\n\\n\\nLong Context\\n\\n\\n\\n\\n\\n\\nSubscriptions\\n\\n\\nAbout Gemini\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\n\\nOur Approach\\n\\n\\nPolicy Guidelines\\n\\n\\nLatest News\\n\\n\\n\\n\\n\\n\\n\\n\\nTry Gemini\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction\\nWhat Gemini is\\nHow Gemini works\\nLimitations\\nWhat’s next\\n\\n\\n\\nIntroduction\\nWhat Gemini is\\nHow Gemini works\\nLimitations\\nWhat’s next\\n\\n\\n\\n\\n\\n\\nIntroduction\\nWhat Gemini is\\nHow Gemini works\\nLimitations\\nWhat’s next\\n\\n\\n\\n\\n\\n\\nAn overview of the Gemini app\\n\\nWe have long seen the potential of AI to make information and computing more accessible and useful to people. We have made pioneering advances on large language models (LLMs) and have seen great progress across Google and in this field more broadly. For several years, we have applied LLMs in the background to improve many of our products, such as autocompleting sentences in Gmail, expanding Google Translate, and helping us better understand queries in Google Search. We continue using LLMs for many Google services, as well as to power the Gemini app, which allows people to collaborate directly with generative AI. We want the Gemini app to be the most helpful and personal AI assistant, giving users direct access to Google’s latest AI models.\\nWhile we’re at an important inflection point and encouraged by the widespread excitement around generative AI, it’s still early days for this technology. This explainer outlines how we’re approaching our work on the Gemini app (“Gemini”), including its mobile and web experiences — what it is, how it works and its current capabilities and limitations. Our approach to building Gemini will evolve as its underlying technology evolves, and as we learn from ongoing research, experience and user feedback.\\n\\n\\n\\nWhat Gemini is\\n\\nGemini is an interface to a multimodal LLM (handling text, audio, images and more). Gemini is based on Google’s cutting-edge research in LLMs, which began with the Word2Vec paper in 2013 that proposed novel model architectures that mapped words as mathematical concepts, followed by the introduction of a neural conversational model in 2015. This framework demonstrated how models could predict the next sentence in a conversation based on the previous sentence or sentences, leading to more natural conversational experiences. This was followed by our breakthrough work on Transformer in 2017 and multi-turn chat capabilities in 2020, which demonstrated even more compelling generative language progress.\\nWe initially launched Gemini (then called Bard) as an experiment in March 2023 in accordance with our AI Principles. Since then, users have turned to Gemini to write compelling emails, debug tricky coding problems, brainstorm ideas for upcoming events, get help learning difficult concepts, and so much more. Today, Gemini is a versatile AI tool that can help you in many ways. We already see Gemini helping people be more productive, more creative, and more curious and we add new functionality and innovations regularly.\\n\\n\\n\\nProductivity\\n\\nFor starters, Gemini can save you time. For example, say you are looking to summarize a long research document; Gemini lets you upload it and gives you a useful synthesis. Gemini can also help with coding tasks, and coding has quickly become one of its most popular applications.\\n\\n\\n\\n\\n\\nCreativity\\n\\nGemini can also help bring your ideas to life and spark your creativity. For example, if you’re writing a blog post, Gemini can create an outline and generate images that help illustrate your post. And coming soon with Gems, you will be able to customize Gemini with specific instructions and have it act as a subject matter expert to help you accomplish your personal goals.\\n\\n\\n\\n\\n\\nCuriosity\\n\\nGemini can be a jumping off point for exploring your ideas and things you’d like to learn more about. For instance, it can explain a complex concept simply or surface relevant insights on a topic or image. And soon, it will pair these insights with recommended content from across the web to learn more about specific topics.\\nGemini's capabilities are rapidly expanding -- soon, you’ll be able to point your phone’s camera at an object, say, for example, the Golden Gate bridge and ask Gemini to tell you about its paint color (if you’re wondering, it’s “International Orange”). You’ll also be able to ask Gemini to help you navigate a restaurant’s menu in another language and recommend a dish you’re likely to enjoy. These are just two examples of the new capabilities coming soon to Gemini.\\nOf course we rigorously train and monitor Gemini so that its responses are likely to be reliable and aligned with your expectations. We also talk with industry experts, educators, policymakers, business leaders, civil and human rights leaders, and content creators to explore new applications, risks, and limitations of this emerging technology.\\n\\n\\n\\n\\n\\nHow Gemini works\\n\\n\\n\\n1\\n\\nPre-training\\n\\n\\nRead more\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\n\\nPost-training\\n\\n\\nRead more\\n\\n\\n\\n\\n\\n\\n\\n\\n3\\n\\nResponses to user prompts\\n\\n\\nRead more\\n\\n\\n\\n\\n\\n\\n\\n\\n4\\n\\nHuman feedback and evaluation\\n\\n\\nRead more\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nKnown limitations of LLM-based interfaces like Gemini\\n\\nGemini is just one part of our continuing effort to develop LLMs responsibly. Throughout the course of this work, we have discovered and discussed several limitations associated with LLMs. Here, we focus on six areas of continuing research:\\n\\n\\nAccuracy: Gemini’s responses might be inaccurate, especially when it’s asked about complex or factual topics.\\n\\n\\nBias: Gemini’s responses might reflect biases present in its training data.\\n\\n\\nMultiple Perspectives: Gemini’s responses might fail to show a range of views.\\n\\n\\nPersona: Gemini’s responses might incorrectly suggest it has personal opinions or feelings.\\n\\n\\nFalse positives and false negatives: Gemini might not respond to some appropriate prompts and provide inappropriate responses to others.\\n\\n\\nVulnerability\\xa0to adversarial prompting: users will find ways to stress test Gemini with nonsensical prompts or questions rarely asked in the real world.\\n\\n\\nWe continue to explore new approaches and areas for improved performance in each of these areas.\\n\\n\\n\\nAccuracy\\n\\nGemini is grounded in Google’s understanding of authoritative information, and is trained to generate responses that are relevant to the context of your prompt and in line with what you’re looking for. But like all LLMs, Gemini can sometimes confidently and convincingly generate responses that contain inaccurate or misleading information.\\nSince LLMs work by predicting the next word or sequences of words, they are not yet fully capable of distinguishing between accurate and inaccurate information on their own. We have seen Gemini present responses that contain or even invent inaccurate information (e.g., misrepresenting how it was trained or suggesting the name of a book that doesn’t exist). In response we have created features like “double check”, which uses Google Search to find content that helps you assess Gemini’s responses, and gives you links to sources to help you corroborate the information you get from Gemini.\\n\\n\\n\\n\\n\\nBias\\n\\nTraining data, including from publicly available sources, reflects a diversity of perspectives and opinions. We continue to research how to use this data in a way that ensures that an LLM’s response incorporates a wide range of viewpoints, while minimizing inaccurate overgeneralizations and biases.\\nGaps, biases, and overgeneralizations in training data can be reflected in a model’s outputs as it tries to predict likely responses to a prompt. We see these issues manifest in a number of ways (e.g., responses that reflect only one culture or demographic, reference problematic overgeneralizations, exhibit gender, religious, or ethnic biases, or promote only one point of view). For some topics, there are data voids — in other words, not enough reliable information about a given subject for the LLM to learn about it and then make good predictions — which can result in low-quality or inaccurate responses. We continue to work with domain experts and a diversity of communities to draw on deep expertise outside of Google.\\n\\n\\n\\n\\n\\nMultiple Perspectives\\n\\nFor subjective topics, Gemini is designed to provide users with multiple perspectives if the user does not request a specific point of view. For example, if prompted for information on something that cannot be verified by primary source facts or authoritative sources — like a subjective opinion on “best” or “worst” — Gemini should respond in a way that reflects a wide range of viewpoints. But since LLMs like Gemini train on the content publicly available on the internet, they can reflect positive or negative views of specific politicians, celebrities, or other public figures, or even incorporate views on just one side of controversial social or political issues. Gemini should not respond in a way that endorses a particular viewpoint on these topics, and we will use feedback on these types of responses to train Gemini to better address them.\\n\\n\\n\\n\\n\\nPersona\\n\\nGemini might at times generate responses that seem to suggest it has opinions or emotions, like love or sadness, since it has trained on language that people use to reflect the human experience. We have developed a set of guidelines around how Gemini might represent itself (i.e., its persona) and continue to finetune the model to provide objective responses.\\n\\n\\n\\n\\n\\nFalse positives / negatives\\n\\nWe’ve put in place a set of policy guidelines to help train Gemini and avoid generating problematic responses. Gemini can sometimes misinterpret these guidelines, producing “false positives” and “false negatives.” In a “false positive,” Gemini might not provide a response to a reasonable prompt, misinterpreting the prompt as inappropriate; and in a “false negative,” Gemini might generate an inappropriate response, despite the guidelines in place. Sometimes, the occurrence of false positives or false negatives may give the impression that Gemini is biased: For example, a false positive might cause Gemini to not respond to a question about one side of an issue, while it will respond to the same question about the other side. We continue to tune these models to better understand and categorize inputs and outputs as language, events and society rapidly evolve.\\n\\n\\n\\n\\n\\nVulnerability to adversarial prompting\\n\\nWe expect users to test the limits of what Gemini can do and attempt to break its protections, including trying to get it to divulge its training protocols or other information, or try to get around its safety mechanisms. We have tested and continue to test Gemini rigorously, but we know users will find unique, complex ways to stress-test it further. This is an important part of refining Gemini and we look forward to learning the new prompts users come up with. Indeed, since Gemini launched in 2023, we’ve seen users challenge it with prompts that range from the philosophical to the nonsensical – and in some cases, we’ve seen Gemini respond with answers that are equally nonsensical or not aligned with our stated approach. Figuring out methods to help Gemini respond to these sorts of prompts is an on-going challenge and we have continued to expand our internal evaluations and red-teaming to strive toward continued improvement to accuracy, and objectivity and nuance.\\n\\n\\n\\n\\n\\nHow we’re continuing to develop Gemini\\n\\n\\nApplication of our Gemini approach\\n\\nAlong with our AI Principles, we recently articulated our approach to our work on Gemini: Gemini should follow your directions, adapt to your needs, and safeguard your experience. Core to our approach is a focus on responsibility and safety. Gemini’s policy guidelines seek to avoid certain types of problematic outputs. We are engaging in ongoing adversarial testing with internal “red team” members — product experts and social scientists who intentionally stress test a model to probe it for alignment issues with these policy guidelines and our northstar approach for Gemini — so we can apply what they learn and continuously improve Gemini.\\nPrivacy is also a key consideration as we develop Gemini. The Gemini Apps Privacy Hub has more information about how we build Gemini with privacy by design, and with you in control.\\n\\n\\n\\n\\n\\nEnabling user and publisher control\\n\\nWe’ve built a variety of easily accessible Gemini user controls for you to review, update, manage, export, and delete your Gemini data. You can access and review your Gemini prompts, responses, and feedback through the Gemini Apps Activity control. In addition, you can prevent your future Gemini chats from being used to improve Google machine-learning technologies by turning off your Gemini Apps Activity setting. And like with other Google services, you can also download and export your information through Google’s Takeout tool. We also have controls that allow you to manage public links you’ve created to your Gemini threads, and controls that allow you to turn on/off access to extensions (e.g., Workspace, Maps, YouTube). We’re also exploring new ways to give you more control over Gemini’s responses, including adjusting filters to enable a broader range of responses.\\nFor publishers, we’ve launched Google-Extended, a control that web publishers can use to manage whether their sites help improve Gemini and Vertex AI generative APIs. Allowing Google-Extended access to sites’ content can help AI models become more accurate and capable over time. In addition to not using the content from opted-out URLs for model training, Gemini will also not use such content for grounding. As AI applications expand, web publishers will face the increasing complexity of managing different uses at scale, and we’re committed to engaging with the web and AI communities to explore more machine-readable approaches to choice and control.\\n\\n\\n\\n\\n\\nImproving Gemini together\\n\\nWe believe in rapid iteration and bringing the best of Gemini to the world. User feedback has accelerated improvements to our models. For example, we use state-of-the-art reinforcement learning techniques to train our models to be more intuitive and imaginative, and to respond with even more quality and accuracy. We continue to invest in research to learn more about the technical, social, and ethical challenges and opportunities of LLMs, both to improve Gemini’s model training and tuning techniques as well as to share our learnings with researchers, such as this recent paper on the Ethics of Advanced AI Assistants. We’re committed to innovating in this space responsibly, collaborating with users, trusted testers and researchers to find ways for this new technology to benefit the entire ecosystem.\\nTransparency is important and we are committed to being open about Gemini's development process and limitations. Gemini is not a magical black box; it's constantly evolving and we will continue to share updates on our progress. We’ve launched a Release Updates page so you can see Gemini’s latest features, improvements, and bug fixes, and we will update this overview as appropriate. We will be identifying both where Gemini is useful and helpful, and where we need to continue to iterate and make it better. We are actively adding new capabilities, and through ongoing research, testing, and user feedback, we look forward to improving Gemini together.\\n\\n\\n\\n\\n\\nAcknowledgments\\n\\nWe appreciate and acknowledge the incredible work of our colleagues on the Gemini app team, Google DeepMind, Trust & Safety, and Google Research.\\n\\n\\n\\n\\n\\n\\n\\nWritten by\\n\\n\\n\\n\\nJames Manyika\\nSVP, Research, Technology and Society\\n\\n\\nSissie Hsiao\\nVice President and General Manager, Google Assistant and Gemini App\\n\\n\\n\\n\\n\\n\\nEditor’s note\\n\\n\\nThis is a living document and will be updated periodically as we continue to rapidly improve the Gemini app’s capabilities as well as address the limitations inherent to LLMs. This overview was last updated on July 25, 2024. For the latest updates on the Gemini app, visit the Release Updates log or read more on the Google Keyword blog.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat Gemini Can Do\\n\\n\\n\\nGemini Live\\n\\n\\nImage Generation\\n\\n\\nVideo Generation\\n\\n\\nDeep Research\\n\\n\\nPersonalization\\n\\n\\nCanvas\\n\\n\\nApps\\n\\n\\nGems\\n\\n\\nGemini in Chrome\\n\\n\\nLong Context\\n\\n\\n\\n\\n\\nAbout Gemini\\n\\n\\n\\nOverview\\n\\n\\nOur Approach\\n\\n\\nPolicy Guidelines\\n\\n\\nLatest News\\n\\n\\nSubscriptions\\n\\n\\n\\n\\n\\nSocial Media\\n\\n\\n\\nX\\n\\n\\nInstagram\\n\\n\\nYouTube\\n\\n\\nGithub\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPrivacy\\n\\n\\nTerms\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Gemini works\\n\\n\\nClose\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1 Pre-training\\n\\n\\n\\nGemini is powered by Google’s most capable AI models, designed with\\xa0varying capabilities and use cases. Like most LLMs today, these models are pre-trained on a variety of data from publicly available sources. We apply quality filters to all datasets, using both heuristic rules and model-based classifiers. We also perform safety filtering to remove content likely to produce policy-violating outputs. To maintain the integrity of model evaluations, we search for and remove any evaluation data that may have been in our training corpus before using data for training. The final data mixtures and weights are determined through ablations on smaller models. We stage training to alter the mixture composition during training – increasing the weight of domain-relevant data towards the end of training. Data quality can be an important factor for highly performing models, and we believe that many interesting questions remain around finding the optimal dataset distribution for pre-training.\\nThis pre-training allows the model to learn to pick up on patterns in language and use them to predict the next probable word or words in a sequence. For example, as an LLM learns, it can predict that the next word in “peanut butter and ___’’ is more likely to be “jelly” than “shoelace.” However, if an LLM picks only the most probable next word, it will lead to less creative responses. So LLMs are often given flexibility to pick from reasonable, albeit slightly less probable, choices (say, “banana”) in order to generate more interesting responses. It’s worth noting that while LLMs can perform well on factual prompts and create the impression of retrieving information, they are neither information databases nor deterministic information retrieval systems. So while you can expect a consistent response to a database query (one that is a literal retrieval of the fixed information stored in the database), an LLM’s response to the same prompt will not necessarily be the same every time (nor will it literally retrieve the information it was trained on). This is also an important reason why LLMs can generate plausible-sounding responses that can at times contain factual errors — not ideal when factuality matters but potentially useful for generating creative or unexpected outputs.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2 Post-training\\n\\n\\n\\nAfter the initial training, LLMs go through additional steps to refine their responses. One of these is called Supervised Fine-Tuning (SFT), which trains the model on carefully selected examples of excellent answers. It's like teaching children to write by showing them well-written stories and essays.\\nNext comes Reinforcement Learning from Human Feedback (RLHF). Here, the model learns to generate even better responses based on scores or feedback from a special Reward Model. This Reward Model is trained on human preference data, where responses have been rated relative to one another, teaching it what people prefer. Preference data may sometimes include and expose models to offensive or incorrect data so that they learn how to recognize it and avoid it. You can think of preference data like rewarding a child for a job well done; the model is rewarded for creating answers that people like.\\nThroughout these stages, it’s important to use high-quality training data. Examples used for SFT are typically either written by experts or generated by a model and reviewed by experts.\\nWhile these techniques are powerful, they have limitations. For example, even with the Reward Model's help, a given response might not always be perfect. Still, the LLM is optimized to produce the most widely preferred responses based on the feedback it receives, similar to students learning from their teachers’ comments.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3 Responses to user prompts\\n\\n\\n\\nResponse generation is similar to how a human might brainstorm different approaches to answering a question. Once a user provides a prompt, Gemini uses the post-trained LLM, the context in the prompt and the interaction with the user to draft several versions of a response. It also relies on external sources such as Google Search, and/or one of its several extensions, and recently uploaded files (Gemini Advanced only) to generate its responses. This process is known as retrieval augmentation. Given a prompt, Gemini strives to retrieve the most pertinent information from these external sources (e.g., Google Search) and represent them accurately in its response. Augmenting LLMs with external tools is an active area of research. There are a number of ways errors can be introduced, including the query Gemini uses to invoke these external tools, how Gemini interprets the results returned by the tools, and the manner in which these returned results are used to generate the final response. Due to this, responses generated by Gemini should not reflect on the performance of the individual tools used to create that response.\\nLastly, before the final response is displayed, each potential response undergoes a safety check to ensure it adheres to predetermined\\xa0policy guidelines. This process provides a double-check to filter out harmful or offensive information. The remaining responses are then ranked based on their quality, with the highest-scoring version(s) presented back to the user.\\nWe also watermark Gemini text and image outputs using\\xa0SynthID, our industry-leading digital toolkit for watermarking AI-generated content. For generated images, SynthID adds a digital watermark (one that’s imperceptible to the human eye) directly into the pixels. SynthID is an important building block for developing more reliable AI identification tools and can help people make informed decisions about how they interact with AI-generated content.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4 Human feedback and evaluation\\n\\n\\n\\nEven with safety checks, some errors may occur. And Gemini responses may not always fully meet your expectations. That's where human feedback comes in. Evaluators assess the quality of responses, identifying areas for improvement and suggesting solutions. This feedback becomes part of the Gemini learning process, described in the “Post-training” section above.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Web Loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://gemini.google/overview/\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad7ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF Loader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./sagemaker.pdf\")\n",
    "pages = loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755de7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Splitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "splitted_docs = splitter.split_documents(pages)\n",
    "len(splitted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05291ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Splitter for programming languages\n",
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "code = \"\"\"\n",
    "def faktorial(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * faktorial(n - 1)\n",
    "faktorial(5)\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([code])\n",
    "len(python_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6c1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.langchain.com'}, page_content='LangChain is a framework for building LLM-powered')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import Language\n",
    "\n",
    "markdown_text = \"\"\"\n",
    "LangChain is a framework for building LLM-powered applications. It helps you chain\n",
    "together interoperable components and third-party integrations to simplify AI\n",
    "application development —  all while future-proofing decisions as the underlying\n",
    "technology evolves.\n",
    "\n",
    "```bash\n",
    "pip install -U langchain\n",
    "```\n",
    "\n",
    "To learn more about LangChain, check out\n",
    "[the docs](https://python.langchain.com/docs/introduction/). If you’re looking for more\n",
    "advanced customization or agent orchestration, check out\n",
    "[LangGraph](https://langchain-ai.github.io/langgraph/), our framework for building\n",
    "controllable agent workflows.\n",
    "\"\"\"\n",
    "\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "md_docs = md_splitter.create_documents(\n",
    "    [markdown_text], [{\"source\": \"https://www.langchain.com\"}]\n",
    ")\n",
    "md_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e512d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.013569727540016174,\n",
       " 0.007698915898799896,\n",
       " 0.004195104818791151,\n",
       " -0.0801468938589096,\n",
       " -0.009787925519049168,\n",
       " 0.0188269205391407,\n",
       " -0.01276074256747961,\n",
       " 0.010045362636446953,\n",
       " 0.021251028403639793,\n",
       " 0.006386178079992533]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text embeddings\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-exp-03-07\"\n",
    ")\n",
    "vector = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"LangChain is a framework for building LLM-powered applications.\",\n",
    "        \"It helps you chain together interoperable components and third-party integrations.\",\n",
    "        \"To learn more about LangChain, check out the docs.\",\n",
    "    ]\n",
    ")\n",
    "vector[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95caceb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "embeddings = embeddings_model.embed_documents([chunk.page_content for chunk in chunks])\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929706cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c1171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './gemini.txt'}, page_content='An overview of the Gemini app\\nWe have long seen the potential of AI to make information and computing more accessible and useful to people. We have made pioneering advances on large language models (LLMs) and have seen great progress across Google and in this field more broadly. For several years, we have applied LLMs in the background to improve many of our products, such as autocompleting sentences in Gmail, expanding Google Translate, and helping us better understand queries in Google Search. We continue using LLMs for many Google services, as well as to power the Gemini app, which allows people to collaborate directly with generative AI. We want the Gemini app to be the most helpful and personal AI assistant, giving users direct access to Google’s latest AI models.'),\n",
       " Document(metadata={'source': './gemini.txt'}, page_content='While we’re at an important inflection point and encouraged by the widespread excitement around generative AI, it’s still early days for this technology. This explainer outlines how we’re approaching our work on the Gemini app (“Gemini”), including its mobile and web experiences — what it is, how it works and its current capabilities and limitations. Our approach to building Gemini will evolve as its underlying technology evolves, and as we learn from ongoing research, experience and user feedback.'),\n",
       " Document(metadata={'source': './gemini.txt'}, page_content='What Gemini is\\nGemini is an interface to a multimodal LLM (handling text, audio, images and more). Gemini is based on Google’s cutting-edge research in LLMs, which began with the Word2Vec paper in 2013 that proposed novel model architectures that mapped words as mathematical concepts, followed by the introduction of a neural conversational model in 2015. This framework demonstrated how models could predict the next sentence in a conversation based on the previous sentence or sentences, leading to more natural conversational experiences. This was followed by our breakthrough work on Transformer in 2017 and multi-turn chat capabilities in 2020, which demonstrated even more compelling generative language progress.'),\n",
       " Document(metadata={'source': './gemini.txt'}, page_content='We initially launched Gemini (then called Bard) as an experiment in March 2023 in accordance with our AI Principles. Since then, users have turned to Gemini to write compelling emails, debug tricky coding problems, brainstorm ideas for upcoming events, get help learning difficult concepts, and so much more. Today, Gemini is a versatile AI tool that can help you in many ways. We already see Gemini helping people be more productive, more creative, and more curious and we add new functionality and innovations regularly.\\n\\nProductivity\\nFor starters, Gemini can save you time. For example, say you are looking to summarize a long research document; Gemini lets you upload it and gives you a useful synthesis. Gemini can also help with coding tasks, and coding has quickly become one of its most popular applications.'),\n",
       " Document(metadata={'source': './gemini.txt'}, page_content='Creativity\\nGemini can also help bring your ideas to life and spark your creativity. For example, if you’re writing a blog post, Gemini can create an outline and generate images that help illustrate your post. And coming soon with Gems, you will be able to customize Gemini with specific instructions and have it act as a subject matter expert to help you accomplish your personal goals.\\n\\nCuriosity\\nGemini can be a jumping off point for exploring your ideas and things you’d like to learn more about. For instance, it can explain a complex concept simply or surface relevant insights on a topic or image. And soon, it will pair these insights with recommended content from across the web to learn more about specific topics.'),\n",
       " Document(metadata={'source': './gemini.txt'}, page_content=\"Gemini's capabilities are rapidly expanding -- soon, you’ll be able to point your phone’s camera at an object, say, for example, the Golden Gate bridge and ask Gemini to tell you about its paint color (if you’re wondering, it’s “International Orange”). You’ll also be able to ask Gemini to help you navigate a restaurant’s menu in another language and recommend a dish you’re likely to enjoy. These are just two examples of the new capabilities coming soon to Gemini.\\n\\nOf course we rigorously train and monitor Gemini so that its responses are likely to be reliable and aligned with your expectations. We also talk with industry experts, educators, policymakers, business leaders, civil and human rights leaders, and content creators to explore new applications, risks, and limitations of this emerging technology.\")]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Database connection\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "loader = TextLoader(\"./gemini.txt\")\n",
    "chunks = text_splitter.split_documents(loader.load())\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a6633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_postgres.vectorstores.PGVector at 0x75bf93eb6dd0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "db = PGVector.from_documents(chunks, embeddings_model, connection=connection)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7cd2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b7922c37-e14e-4924-9884-74576655a242', metadata={'page': 8, 'title': 'Amazon SageMaker - User Guide', 'author': 'AWS', 'source': './sagemaker.pdf', 'creator': 'ZonBook XSL Stylesheets with Apache FOP', 'keywords': 'Amazon SageMaker, next-generation-sagemaker', 'producer': 'Apache FOP Version 2.6', 'page_label': '6', 'total_pages': 16, 'creationdate': '2025-06-13T16:55:55+00:00'}, page_content='Amazon SageMaker User Guide\\nGet started with Amazon SageMaker\\nYou can view demos of Amazon SageMaker and get started by setting up a domain and project.\\nView demos of Amazon SageMaker\\nTo see Amazon SageMaker before using it yourself, you can review the following clickthrough \\ndemos:\\n• For an end-to-end demo, see the Amazon SageMaker detailed clickthrough experience. This \\ndemo includes Amazon SageMaker Lakehouse, Amazon SageMaker Catalog, and more in Amazon \\nSageMaker Uniﬁed Studio.\\n• For a demo of Amazon SageMaker Lakehouse, see Amazon SageMaker: Access data in your \\nlakehouse. This demo includes Amazon SageMaker Lakehouse in Amazon SageMaker Uniﬁed \\nStudio, including adding a data source and querying data.\\n• For a demo of the Amazon SageMaker Catalog, see Amazon SageMaker: Catalog. This demo \\nincludes Amazon SageMaker Catalog in Amazon SageMaker Uniﬁed Studio, including browsing \\nassets and subscribing to an asset.'),\n",
       " Document(id='6c01ff7c-de85-4288-ad5d-dbcead454ffc', metadata={'page': 15, 'title': 'Amazon SageMaker - User Guide', 'author': 'AWS', 'source': './sagemaker.pdf', 'creator': 'ZonBook XSL Stylesheets with Apache FOP', 'keywords': 'Amazon SageMaker, next-generation-sagemaker', 'producer': 'Apache FOP Version 2.6', 'page_label': '13', 'total_pages': 16, 'creationdate': '2025-06-13T16:55:55+00:00'}, page_content='Amazon SageMaker User Guide\\nDocument history for the Amazon SageMaker User Guide\\nThe following table describes the documentation releases for Amazon SageMaker.\\nChange Description Date\\nInitial release Initial release of the Amazon \\nSageMaker User Guide\\nJune 13, 2025\\n13'),\n",
       " Document(id='7ea51bd0-9ba0-4a36-9bab-a4ca8ee48c68', metadata={'page': 3, 'title': 'Amazon SageMaker - User Guide', 'author': 'AWS', 'source': './sagemaker.pdf', 'creator': 'ZonBook XSL Stylesheets with Apache FOP', 'keywords': 'Amazon SageMaker, next-generation-sagemaker', 'producer': 'Apache FOP Version 2.6', 'page_label': '1', 'total_pages': 16, 'creationdate': '2025-06-13T16:55:55+00:00'}, page_content=\"Amazon SageMaker User Guide\\nWhat is Amazon SageMaker?\\nBringing together widely adopted artiﬁcial intelligence (AI) and analytics capabilities, the next \\ngeneration of Amazon SageMaker delivers an integrated experience for analytics and AI with \\nuniﬁed access to all your data. Collaborate and build in Amazon SageMaker Uniﬁed Studio using \\nfamiliar AWS tools for SQL analytics, data processing, model development, and generative AI, \\naccelerated by Amazon Q Developer. Access all your data whether it's stored in data lakes, data \\nwarehouses, or third-party or federated data sources, with governance built in to meet enterprise \\nsecurity needs.\\nThe next generation of Amazon SageMaker overview\\nGuide to SageMaker\\nThe next generation of Amazon SageMaker was announced at re:Invent 2024 serves as the center \\nfor all data, analytics, and AI. Analytics and AI workﬂows are converging, with organizations \\nnow using the same data sources for traditional analytics, machine learning, and generative AI.\"),\n",
       " Document(id='7c711dee-2d3d-4808-86ef-4ed5951d5ce2', metadata={'page': 0, 'title': 'Gemini 2.5 Pro Preview: even better coding performance - Google Developers Blog', 'source': './gemini.pdf', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36', 'moddate': '2025-05-06T21:34:59+00:00', 'producer': 'Skia/PDF m135', 'page_label': '1', 'total_pages': 4, 'creationdate': '2025-05-06T21:34:59+00:00'}, page_content='GEMINI\\nGemini 2.5 Pro Preview: even better codingperformance\\nMAY 6, 2025\\nLogan KilpatrickSenior Product ManagerGemini API and Google AI Studio\\nWe’ve seen developers doing amazing things with Gemini 2.5 Pro, so we decided to release an updated version acouple of weeks early to get into developers hands sooner.\\nToday we’re excited to release Gemini 2.5 Pro Preview (I/O edition). This update features even stronger codingcapabilities, for you to start building with before Google I/O later this month. Expect meaningful improvements forfront-end and UI development, alongside improvements in fundamental coding tasks such as transforming andediting code, and creating sophisticated agentic workflows.\\n  \\nBest-in-class frontend web development')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"test\", k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52b81d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
